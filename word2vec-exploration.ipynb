{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d01705",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005ee26",
   "metadata": {},
   "source": [
    "### Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f7086",
   "metadata": {},
   "source": [
    "Em machine learning, o objetivo é empregar o poder computacional para resolver problemas do mundo real. Diferentemente dos humanos, que possuem percepção direta, os computadores operam fundamentalmente com números, convertidos em última instância para sequências de __0's__ e __1's__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16abdb",
   "metadata": {},
   "source": [
    "Atualmente, algoritmos processam diversos tipos de dados, incluindo números, imagens, áudios e textos. Para que um computador possa trabalhar com essas informações, é necessário encontrar maneiras de representá-las numericamente, permitindo sua manipulação. No contexto do texto, existem várias técnicas capazes de convertê-lo em informação numérica. Essas técnicas variam desde abordagens mais simples, como a contagem de palavras no modelo _Bag of Words_, até modelos mais sofisticados, como os _Transformers_. Este estudo se concentra na técnica __Word2Vec__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534e361",
   "metadata": {},
   "source": [
    "O __Word2Vec__ consiste em representar palavras em um espaço vetorial de _n-dimensões_, de forma que essa representação capture seu significado semântico. Geralmente, palavras semanticamente relacionadas, como \"mulher\" e \"garota\", terão representações vetoriais próximas nesse espaço. Em contraste, a similaridade vetorial entre palavras menos relacionadas, como \"mulher\" e \"homem\", será menor. Essa lógica se aplica a outros exemplos de palavras como visualizado abaixo.\n",
    "> Word2vec é um algoritmo para obter word embeddings treinando uma rede neural rasa (com apenas uma hidden layer) com duas arquiteturas possíveis: CBOW ou Skip-Gram. ([Word Embedding: fazendo o computador entender o significado das palavras](https://medium.com/turing-talks/word-embedding-fazendo-o-computador-entender-o-significado-das-palavras-92fe22745057))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f438ad0",
   "metadata": {},
   "source": [
    "![Representação Vetorial de Palavras em um Plano 3D](https://miro.medium.com/v2/resize:fit:868/0*Cgod6JuBcJyd9GVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b5488",
   "metadata": {},
   "source": [
    "### Arquiteturas do Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdec4cf",
   "metadata": {},
   "source": [
    "Antes de iniciar a exploração, vou importar algumas bibliotecas necessárias que utilizarei para esse projeto (evitar importações desnecessárias ou repetitivas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5b88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1e068",
   "metadata": {},
   "source": [
    "Além disso, vou criar algumas funções comuns que vou utilizar nesse notebook python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Funtion to download zip file from URL\n",
    "def download_file_zip(URL: str, file_name: str):\n",
    "    os.makedirs(\"data/model/zipfiles\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Faz o download\n",
    "        response = requests.get(URL)\n",
    "        response.raise_for_status()  # Lança erro se status != 200\n",
    "        \n",
    "        # Salva o arquivo\n",
    "        file_path = os.path.join(\"data\", \"model\", \"zipfiles\", f\"{file_name}.zip\")\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        return f\"data/model/zipfiles/{file_name}.zip\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {str(e)}\")\n",
    "\n",
    "\n",
    "def unzip_file(source_path: str, final_path: str):\n",
    "    with zipfile.ZipFile(source_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(final_path)\n",
    "        print(\"All unzip!\")  # Substitua pelo caminho desejado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db5032",
   "metadata": {},
   "source": [
    "#### CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8443d7",
   "metadata": {},
   "source": [
    "A arquitetura CBOW (Continuous Bag-of-Words) tem como objetivo prever uma palavra central com base no contexto das palavras que a cercam. Ela utiliza mais processamento uma vez que ela necessita analisar as palavras ao redor para que ela busque qual melhor palavra que se encaixa naquele contexto.\n",
    "Na palavra abaixo:\n",
    "> Eu vou para a __________ estudar com a professora!\n",
    "\n",
    "O algoritmo analisa as palavras ao redor da palavra buscada e por exemplo, identifica que estudar tem relação com outras palavras como __casa__, __escola__, __biblioteca,__ dentre outras haver com esse contexto. Quando ele analisa as outras palavras da frase ele vê que __professora__ está mais relacionado com __escola__ do que com __biblioteca__ por exemplo e com base nessa ideia ele indica de maneira probabilística que a melhor palavra indicada para esse caso seja __escola__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfa359",
   "metadata": {},
   "source": [
    "Em geral, algumas __vantagens__ que acompanha esse tipo de técnica é que o CBOW converge mais rapidamente do que o Skip-gram, pois precisa prever uma única palavra central a partir de múltiplos contextos, o que torna o problema de aprendizado um pouco mais fácil. Além disso, ele apresenta boa representação para palavras frequentes já que CBOW tende a aprender boas representações para palavras que aparecem com frequência no corpus, pois se beneficia da agregação de informações de múltiplos contextos. Além disso, ele é menos sensível a palavras raras já que como ele utiliza o contexto para prever a palavra central, o impacto de palavras raras no treinamento geral pode ser menor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aab796",
   "metadata": {},
   "source": [
    "##### Explorando um pouco a arquiterura CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963d7ba",
   "metadata": {},
   "source": [
    "Baixando o modelo CBOW treinado para português brasileiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5869c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download concluído: data\\model\\zipfiles\\cbow_ptbr_100d.zip\n"
     ]
    }
   ],
   "source": [
    "URL_CBOW_100D = \"http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s100.zip\"\n",
    "\n",
    "path_final = download_file_zip(URL_CBOW_100D, \"cbow_ptbr_100d\")\n",
    "unzip_file(path_final, \"data/model/cbow_ptbr_100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('data/model/cbow_ptbr_100d/cbow_s100.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8846c",
   "metadata": {},
   "source": [
    "### Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f963bd84",
   "metadata": {},
   "source": [
    "- [Word2Vec e sua importância na etapa de pré-processamento](https://medium.com/@everton.tomalok/word2vec-e-sua-importância-na-etapa-de-pré-processamento-d0813acfc8ab)\n",
    "- [Word Embedding: fazendo o computador entender o significado das palavras](https://medium.com/turing-talks/word-embedding-fazendo-o-computador-entender-o-significado-das-palavras-92fe22745057)\n",
    "- [What Is Word2Vec and How Does It Work?](https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27827dcb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
