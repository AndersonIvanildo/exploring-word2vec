{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d01705",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005ee26",
   "metadata": {},
   "source": [
    "### Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f7086",
   "metadata": {},
   "source": [
    "Em machine learning, o objetivo é empregar o poder computacional para resolver problemas do mundo real. Diferentemente dos humanos, que possuem percepção direta, os computadores operam fundamentalmente com números, convertidos em última instância para sequências de __0's__ e __1's__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16abdb",
   "metadata": {},
   "source": [
    "Atualmente, algoritmos processam diversos tipos de dados, incluindo números, imagens, áudios e textos. Para que um computador possa trabalhar com essas informações, é necessário encontrar maneiras de representá-las numericamente, permitindo sua manipulação. No contexto do texto, existem várias técnicas capazes de convertê-lo em informação numérica. Essas técnicas variam desde abordagens mais simples, como a contagem de palavras no modelo _Bag of Words_, até modelos mais sofisticados, como os _Transformers_. Este estudo se concentra na técnica __Word2Vec__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534e361",
   "metadata": {},
   "source": [
    "O __Word2Vec__ consiste em representar palavras em um espaço vetorial de _n-dimensões_, de forma que essa representação capture seu significado semântico. Geralmente, palavras semanticamente relacionadas, como \"mulher\" e \"garota\", terão representações vetoriais próximas nesse espaço. Em contraste, a similaridade vetorial entre palavras menos relacionadas, como \"mulher\" e \"homem\", será menor. Essa lógica se aplica a outros exemplos de palavras como visualizado abaixo.\n",
    "> Word2vec é um algoritmo para obter word embeddings treinando uma rede neural rasa (com apenas uma hidden layer) com duas arquiteturas possíveis: CBOW ou Skip-Gram. ([Word Embedding: fazendo o computador entender o significado das palavras](https://medium.com/turing-talks/word-embedding-fazendo-o-computador-entender-o-significado-das-palavras-92fe22745057))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f438ad0",
   "metadata": {},
   "source": [
    "![Representação Vetorial de Palavras em um Plano 3D](https://miro.medium.com/v2/resize:fit:868/0*Cgod6JuBcJyd9GVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b5488",
   "metadata": {},
   "source": [
    "### Arquiteturas do Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdec4cf",
   "metadata": {},
   "source": [
    "Antes de iniciar a exploração, vou importar algumas bibliotecas necessárias que utilizarei para esse projeto (evitar importações desnecessárias ou repetitivas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1e068",
   "metadata": {},
   "source": [
    "Além disso, vou criar algumas funções comuns que vou utilizar nesse notebook python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f5d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Funtion to download zip file from URL\n",
    "def download_file_zip(URL: str, file_name: str):\n",
    "    os.makedirs(\"data/model/zipfiles\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Faz o download\n",
    "        response = requests.get(URL)\n",
    "        response.raise_for_status()  # Lança erro se status != 200\n",
    "        \n",
    "        # Salva o arquivo\n",
    "        file_path = os.path.join(\"data\", \"model\", \"zipfiles\", f\"{file_name}.zip\")\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        return f\"data/model/zipfiles/{file_name}.zip\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {str(e)}\")\n",
    "\n",
    "\n",
    "def unzip_file(source_path: str, final_path: str):\n",
    "    with zipfile.ZipFile(source_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(final_path)\n",
    "        print(\"All unzip!\")  # Substitua pelo caminho desejado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db5032",
   "metadata": {},
   "source": [
    "#### CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8443d7",
   "metadata": {},
   "source": [
    "A arquitetura CBOW (Continuous Bag-of-Words) tem como objetivo prever uma palavra central com base no contexto das palavras que a cercam. Ela utiliza mais processamento uma vez que ela necessita analisar as palavras ao redor para que ela busque qual melhor palavra que se encaixa naquele contexto.\n",
    "Na palavra abaixo:\n",
    "> Eu vou para a __________ estudar com a professora!\n",
    "\n",
    "O algoritmo analisa as palavras ao redor da palavra buscada e por exemplo, identifica que estudar tem relação com outras palavras como __casa__, __escola__, __biblioteca,__ dentre outras haver com esse contexto. Quando ele analisa as outras palavras da frase ele vê que __professora__ está mais relacionado com __escola__ do que com __biblioteca__ por exemplo e com base nessa ideia ele indica de maneira probabilística que a melhor palavra indicada para esse caso seja __escola__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfa359",
   "metadata": {},
   "source": [
    "Em geral, algumas __vantagens__ que acompanha esse tipo de técnica é que o CBOW converge mais rapidamente do que o Skip-gram, pois precisa prever uma única palavra central a partir de múltiplos contextos, o que torna o problema de aprendizado um pouco mais fácil. Além disso, ele apresenta boa representação para palavras frequentes já que CBOW tende a aprender boas representações para palavras que aparecem com frequência no corpus, pois se beneficia da agregação de informações de múltiplos contextos. Além disso, ele é menos sensível a palavras raras já que como ele utiliza o contexto para prever a palavra central, o impacto de palavras raras no treinamento geral pode ser menor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aab796",
   "metadata": {},
   "source": [
    "##### Explorando um pouco a arquiterura CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963d7ba",
   "metadata": {},
   "source": [
    "Baixando o modelo CBOW treinado para português brasileiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5869c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unzip!\n"
     ]
    }
   ],
   "source": [
    "URL_CBOW_100D = \"http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s100.zip\"\n",
    "\n",
    "path_final = download_file_zip(URL_CBOW_100D, \"cbow_ptbr_100d\")\n",
    "unzip_file(path_final, \"data/model/cbow_ptbr_100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow = KeyedVectors.load_word2vec_format('data/model/cbow_ptbr_100d/cbow_s100.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bca18d",
   "metadata": {},
   "source": [
    "Um exemplo de como o modelo representa as palavras, o termo __carro__, é representado nesse modelo de 100 dimensões com o vetor abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1b437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.133066 -0.140199 -0.123593 -0.327289  0.527658 -0.093072  0.211601\n",
      " -0.080882  0.136657  0.113021 -0.047345  0.193672 -0.343223  0.140333\n",
      "  0.114996  0.302133 -0.276257 -0.019003 -0.170397  0.006797  0.26281\n",
      "  0.48911  -0.259811  0.565083  0.19642   0.107387 -0.01361  -0.128821\n",
      " -0.142525  0.145407 -0.143249 -0.17207   0.019843 -0.181988  0.277796\n",
      "  0.159842  0.233585 -0.211342  0.094705  0.132454  0.263601  0.212976\n",
      " -0.016437 -0.112659  0.501575 -0.267115 -0.128304  0.054397 -0.240181\n",
      " -0.12104   0.141273 -0.506544  0.21905  -0.302853 -0.463127 -0.002334\n",
      "  0.284288  0.319843  0.468935 -0.363241 -0.191774  0.024671  0.163243\n",
      " -0.099187 -0.025341  0.094963  0.166238  0.149067  0.306253 -0.201536\n",
      " -0.327082  0.035873 -0.096247  0.086678  0.103305 -0.249064 -0.259611\n",
      " -0.472722 -0.485474 -0.344718 -0.216763 -0.188939  0.387807  0.095012\n",
      "  0.148751  0.078313  0.205438 -0.076185 -0.080932 -0.19896  -0.246814\n",
      "  0.007546 -0.12768  -0.071645 -0.040573  0.210866  0.240887 -0.087581\n",
      "  0.425937 -0.167982]\n"
     ]
    }
   ],
   "source": [
    "print(model_cbow['carro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00ad9a",
   "metadata": {},
   "source": [
    "Assim como o termo __veículo__ é representado pelo vetor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137139c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.006231 -0.054839 -0.211529 -0.261954  0.585775 -0.118988  0.226396\n",
      " -0.039966  0.015274  0.131516 -0.105483  0.320252 -0.271816  0.278673\n",
      "  0.09556   0.303267 -0.36492   0.106018 -0.292437 -0.021026  0.239087\n",
      "  0.499897 -0.16466   0.511368  0.062144  0.116286 -0.064749 -0.146283\n",
      " -0.063641  0.15635   0.086804 -0.109098  0.152468 -0.362194  0.28604\n",
      " -0.037003  0.216354 -0.076125  0.099117  0.155871  0.132667 -0.077328\n",
      "  0.054892 -0.163485  0.459812 -0.194224 -0.076284 -0.062852 -0.306793\n",
      " -0.091259  0.281505 -0.49223   0.091281 -0.314142 -0.366766 -0.130913\n",
      "  0.244339  0.316981  0.460022 -0.49401  -0.281279 -0.065548  0.06151\n",
      " -0.026801 -0.04999   0.067487  0.05858  -0.03238   0.201022 -0.028652\n",
      " -0.239572  0.088996 -0.062438  0.119585  0.050447 -0.350612 -0.166851\n",
      " -0.528883 -0.618139 -0.466495 -0.257315 -0.135686  0.347661  0.087382\n",
      "  0.081486  0.080525  0.222738 -0.207515  0.020843 -0.211405 -0.277013\n",
      " -0.06448  -0.21568  -0.00472  -0.156896  0.110703  0.221783  0.037422\n",
      "  0.339916 -0.147342]\n"
     ]
    }
   ],
   "source": [
    "print(model_cbow['veículo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79631a",
   "metadata": {},
   "source": [
    "##### Explorando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0cb064",
   "metadata": {},
   "source": [
    "Uma vez com esse modelo carregado, pode-se utilizar algumas funções da biblioteca para exploração:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02212acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palavra</th>\n",
       "      <th>Similaridade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>veículo</td>\n",
       "      <td>0.924637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caminhão</td>\n",
       "      <td>0.886713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jipe</td>\n",
       "      <td>0.846801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avião</td>\n",
       "      <td>0.831295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>barco</td>\n",
       "      <td>0.813364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>parabrisa</td>\n",
       "      <td>0.799635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>elevador</td>\n",
       "      <td>0.793397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cofre</td>\n",
       "      <td>0.788872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>carrinho</td>\n",
       "      <td>0.787174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>passageiro</td>\n",
       "      <td>0.785835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Palavra  Similaridade\n",
       "0     veículo      0.924637\n",
       "1    caminhão      0.886713\n",
       "2        jipe      0.846801\n",
       "3       avião      0.831295\n",
       "4       barco      0.813364\n",
       "5   parabrisa      0.799635\n",
       "6    elevador      0.793397\n",
       "7       cofre      0.788872\n",
       "8    carrinho      0.787174\n",
       "9  passageiro      0.785835"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar as 10 palavras mais similares do modelo\n",
    "lista_palavras = model_cbow.most_similar('carro', topn=10)\n",
    "lista_mais_similares_carro = pd.DataFrame(lista_palavras, columns=['Palavra', 'Similaridade'])\n",
    "lista_mais_similares_carro.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de5e1a",
   "metadata": {},
   "source": [
    "E qual seria a similaridade entre as palavas __mulher__ e __rainha__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93399e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A similaridade entre as palavras MULHER e RAINHA é 0.4940199851989746\n"
     ]
    }
   ],
   "source": [
    "print(f\"A similaridade entre as palavras MULHER e RAINHA é {model_cbow.similarity('mulher', 'rainha')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17735fb",
   "metadata": {},
   "source": [
    "Descobrindo a palavra dentre um grupo que menos tem relação com as outras do grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a83dbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A palavra do grupo ['maçã', 'banana', 'laranja', 'cachorro'] que menos está relacionado é CACHORRO!\n"
     ]
    }
   ],
   "source": [
    "lista_palavras = [\"maçã\", \"banana\", \"laranja\", \"cachorro\"]\n",
    "palavra_menos_similar = model_cbow.doesnt_match(lista_palavras)\n",
    "print(f\"A palavra do grupo {lista_palavras} que menos está relacionado é {palavra_menos_similar.upper()}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08049a9",
   "metadata": {},
   "source": [
    "#### SKIPGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731475fd",
   "metadata": {},
   "source": [
    "A arquitetura Skip-Gram (Continuous Skip-Gram) tem como objetivo prever as palavras de contexto a partir da palavra central. Ela utiliza mais processamento, pois precisa prever múltiplas palavras de contexto para cada palavra de entrada, calculando probabilidades para cada termo no vocabulário.\n",
    "\n",
    "Na palavra abaixo:\n",
    "\n",
    "> Eu vou para a escola estudar com a ______!\n",
    "\n",
    "O algoritmo toma como entrada a palavra central (por exemplo, estudar) e, a partir dela, tenta prever quais palavras costumam aparecer ao seu redor. Nesse contexto, ele identifica que “estudar” está associado a termos como “casa”, “escola”, “biblioteca”, “professora” etc. Ao processar a frase acima, o Skip-Gram calcula a probabilidade de cada uma dessas palavras aparecer no lugar do blank e, de forma probabilística, indica qual contexto é mais provável naquele ponto (por exemplo, “professora”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebcde0f",
   "metadata": {},
   "source": [
    "Em geral, algumas vantagens que acompanham esse tipo de técnica são uma melhora para palavras raras uma vez que como o modelo prevê múltiplos contextos para cada palavra central, o Skip-Gram aprende representações de alta qualidade mesmo para termos pouco frequentes no corpus. Além disso, essa arquitetura captura nuances dos diferentes contextos em que uma palavra aparece, pois cada ocorrência contribui para ajustar o vetor central."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af1892",
   "metadata": {},
   "source": [
    "##### Explorando um pouco a arquiterura CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1c778",
   "metadata": {},
   "source": [
    "Baixando o modelo SKIPGRAM para o português brasileiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "126340b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unzip!\n"
     ]
    }
   ],
   "source": [
    "URL_SKIPGRAM_100D = \"http://143.107.183.175:22980/download.php?file=embeddings/word2vec/skip_s100.zip\"\n",
    "\n",
    "path_final = download_file_zip(URL_SKIPGRAM_100D, \"skipgram_ptbr_100d\")\n",
    "unzip_file(path_final, \"data/model/skipgram_ptbr_100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac01920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram = KeyedVectors.load_word2vec_format(\"data/model/skipgram_ptbr_100d/skip_s100.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647aaf2",
   "metadata": {},
   "source": [
    "Para os mesmos termos anteriores, agora tem-se a aplicação com a arquitetura SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8c33c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03052  -0.179981  0.318288 -0.420714  0.240731 -0.02601  -0.077635\n",
      "  0.228754  0.129099  0.09437   0.358149 -0.419461 -0.296144  0.243431\n",
      "  0.209637  0.153136 -0.01747   0.005728  0.053692  0.008757  0.224109\n",
      "  0.214447  0.266326 -0.410923  0.111632 -0.412724 -0.322653  0.191501\n",
      "  0.161768  0.308219  0.261416  0.05929  -0.327802 -0.108396 -0.067312\n",
      " -0.374585 -0.292027 -0.032074 -0.088213  0.164639 -0.187844 -0.046748\n",
      "  0.325778 -0.272374 -0.241957 -0.190494 -0.068726  0.182147  0.314323\n",
      "  0.18177  -0.160564 -0.178233  0.291935  0.189777 -0.269338 -0.047054\n",
      " -0.215373  0.008573  0.045078  0.130132 -0.080728 -0.260449 -0.011681\n",
      "  0.119967 -0.219406 -0.17135   0.226871  0.020841  0.084167  0.065814\n",
      "  0.258219  0.439529  0.006809 -0.115177 -0.103107  0.054834  0.698577\n",
      " -0.161539 -0.18658  -0.122717 -0.281636  0.181422 -0.076578 -0.111674\n",
      " -0.141989 -0.874926  0.28169   0.117719  0.267188 -0.063896 -0.406935\n",
      "  0.396001 -0.41795  -0.229335 -0.072306  0.165195 -0.199483  0.244025\n",
      "  0.575772  0.011751]\n"
     ]
    }
   ],
   "source": [
    "print(model_skipgram['carro']) # Vetor que representa o termo 'carro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae60b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.048543 -0.146925  0.14268  -0.550171  0.184191 -0.010326 -0.039762\n",
      "  0.247268  0.05561   0.059013  0.329475 -0.339056 -0.226146  0.126701\n",
      "  0.137502  0.316843 -0.128018  0.136733  0.091175  0.024942  0.200224\n",
      "  0.113542  0.343585 -0.493607  0.093524 -0.587963 -0.506089  0.255164\n",
      "  0.195453  0.359028  0.230092  0.237051 -0.291982 -0.073544  0.122711\n",
      " -0.341275 -0.232137  0.106199 -0.015645  0.009579 -0.396079 -0.109282\n",
      "  0.332166 -0.179727 -0.283452 -0.11684  -0.111095  0.148477  0.256435\n",
      "  0.197933 -0.119023 -0.160942  0.14026   0.232627 -0.301246  0.065198\n",
      " -0.26916  -0.151104  0.066499 -0.036433 -0.243753 -0.116231  0.005079\n",
      "  0.222212 -0.14063  -0.028491  0.187754  0.037357  0.018899  0.132222\n",
      "  0.129842  0.568389 -0.124462 -0.009716 -0.001952  0.224699  0.786892\n",
      " -0.363619 -0.2902   -0.135668 -0.362552  0.076921  0.031073  0.008528\n",
      " -0.025896 -0.82875   0.220282  0.149317  0.455413  0.007924 -0.505413\n",
      "  0.385133 -0.314584 -0.337715 -0.015384  0.178757 -0.126213  0.26852\n",
      "  0.415736 -0.084926]\n"
     ]
    }
   ],
   "source": [
    "print(model_skipgram['veículo']) # Vetor que representa o termo 'veículo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7f6b2",
   "metadata": {},
   "source": [
    "#### Explorando o Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af43fc6",
   "metadata": {},
   "source": [
    "Agora, com a outra arquitetura, seus valores são diferentes pois seu treinamento foi feito com outra técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35fe4b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palavra</th>\n",
       "      <th>Similaridade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>veículo</td>\n",
       "      <td>0.926959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caminhão</td>\n",
       "      <td>0.895195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passageiro</td>\n",
       "      <td>0.849853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jipe</td>\n",
       "      <td>0.823606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>motorista</td>\n",
       "      <td>0.819495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trator</td>\n",
       "      <td>0.814430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>furgão</td>\n",
       "      <td>0.805145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>táxi</td>\n",
       "      <td>0.792833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>motociclista</td>\n",
       "      <td>0.773929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ciclomotor</td>\n",
       "      <td>0.770150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Palavra  Similaridade\n",
       "0       veículo      0.926959\n",
       "1      caminhão      0.895195\n",
       "2    passageiro      0.849853\n",
       "3          jipe      0.823606\n",
       "4     motorista      0.819495\n",
       "5        trator      0.814430\n",
       "6        furgão      0.805145\n",
       "7          táxi      0.792833\n",
       "8  motociclista      0.773929\n",
       "9    ciclomotor      0.770150"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar as 10 palavras mais similares do modelo\n",
    "lista_palavras = model_skipgram.most_similar('carro', topn=10)\n",
    "lista_mais_similares_carro = pd.DataFrame(lista_palavras, columns=['Palavra', 'Similaridade'])\n",
    "lista_mais_similares_carro.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090eede1",
   "metadata": {},
   "source": [
    "Medindo a similaridade entre os termos __mulher__ e __rainha__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "436b196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A similaridade entre as palavras MULHER e RAINHA é 0.5242229700088501\n"
     ]
    }
   ],
   "source": [
    "print(f\"A similaridade entre as palavras MULHER e RAINHA é {model_skipgram.similarity('mulher', 'rainha')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722c251",
   "metadata": {},
   "source": [
    "A mesma prática para descobrir qual o termo menos relacionado. Nesse caso, indepedentemente da arquitetura, a eficiência se mantém."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e9a573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A palavra do grupo ['maçã', 'banana', 'laranja', 'cachorro'] que menos está relacionado é CACHORRO!\n"
     ]
    }
   ],
   "source": [
    "lista_palavras = [\"maçã\", \"banana\", \"laranja\", \"cachorro\"]\n",
    "palavra_menos_similar = model_skipgram.doesnt_match(lista_palavras)\n",
    "print(f\"A palavra do grupo {lista_palavras} que menos está relacionado é {palavra_menos_similar.upper()}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8846c",
   "metadata": {},
   "source": [
    "### Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f963bd84",
   "metadata": {},
   "source": [
    "- [Word2Vec e sua importância na etapa de pré-processamento](https://medium.com/@everton.tomalok/word2vec-e-sua-importância-na-etapa-de-pré-processamento-d0813acfc8ab)\n",
    "- [Word Embedding: fazendo o computador entender o significado das palavras](https://medium.com/turing-talks/word-embedding-fazendo-o-computador-entender-o-significado-das-palavras-92fe22745057)\n",
    "- [What Is Word2Vec and How Does It Work?](https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27827dcb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
